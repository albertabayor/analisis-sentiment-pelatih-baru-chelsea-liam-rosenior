{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preprocessing\n",
    "\n",
    "This notebook performs data cleaning and preprocessing on the raw Twitter dataset.\n",
    "\n",
    "## Steps:\n",
    "1. Load raw dataset\n",
    "2. Remove empty tweets\n",
    "3. Remove duplicate tweets\n",
    "4. Filter out @grok queries\n",
    "5. Filter English language tweets\n",
    "6. Apply text cleaning pipeline\n",
    "7. Add engagement features\n",
    "8. Save cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to Python path for proper module imports\n",
    "project_root = os.path.dirname(os.getcwd())  # Go up one level from notebooks/\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import custom modules\n",
    "from src import utils, preprocessing, feature_engineering, models\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET OVERVIEW\n",
      "============================================================\n",
      "Total tweets: 408\n",
      "\n",
      "Columns: ['Tweet Link', 'Author Handle', 'Tweet Content', 'Views', 'Likes', 'Retweets', 'Replies', 'Tweet Creation Date', 'Scraped Date']\n",
      "\n",
      "Dataset shape: (408, 9)\n",
      "\n",
      "Data types:\n",
      "Tweet Link             object\n",
      "Author Handle          object\n",
      "Tweet Content          object\n",
      "Views                   int64\n",
      "Likes                   int64\n",
      "Retweets                int64\n",
      "Replies                 int64\n",
      "Tweet Creation Date    object\n",
      "Scraped Date           object\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_path = utils.get_data_path('tweets.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET OVERVIEW\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total tweets: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\n{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample raw tweets:\n",
      "                                                                                                    Tweet Content  \\\n",
      "0                                                            We tried to stop it from overthinking.\\n\\nWe failed.   \n",
      "1                                                                                                             Waw   \n",
      "2                                                                                              @grok\\n who is he?   \n",
      "3                                                                @grok\\n make him bald and resemble Enzo Maresca.   \n",
      "4                                                         @grok\\n remove Liam rosenior name and put Arnold Masoha   \n",
      "5  I predicted this\\nThey need some one they can control without him battling and eye.\\nThey need a 'yes sir' man   \n",
      "6                                                                                @grok\\n what is the likes count?   \n",
      "7                                                                             @grok\\n just lol  it will end tears   \n",
      "8                                                                                            @grok\\n who is Liam?   \n",
      "9                                                 @grok\\n who is this guy and what's his achievements in football   \n",
      "\n",
      "   Likes  Retweets  \n",
      "0     53         3  \n",
      "1      0         0  \n",
      "2      0         0  \n",
      "3      0         0  \n",
      "4      0         0  \n",
      "5      1         0  \n",
      "6      0         0  \n",
      "7      0         0  \n",
      "8      0         0  \n",
      "9      0         0  \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Tweet Link              0\n",
      "Author Handle           0\n",
      "Tweet Content          15\n",
      "Views                   0\n",
      "Likes                   0\n",
      "Retweets                0\n",
      "Replies                 0\n",
      "Tweet Creation Date     0\n",
      "Scraped Date            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display sample tweets\n",
    "print(\"Sample raw tweets:\")\n",
    "print(df[['Tweet Content', 'Likes', 'Retweets']].head(10))\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\n\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Data Preprocessing Pipeline ===\n",
      "\n",
      "Initial dataset: 408 tweets\n",
      "\n",
      "✓ Removed 15 empty tweets\n",
      "✓ Removed 4 duplicate tweets\n",
      "✓ Removed 19 @grok query tweets\n",
      "Detecting tweet languages...\n",
      "✓ Removed 41 non-English tweets\n",
      "✓ Remaining English tweets: 329\n",
      "✓ Added engagement and text features\n",
      "\n",
      "=== Preprocessing Complete ===\n",
      "Final dataset: 329 tweets\n",
      "\n",
      "\n",
      "============================================================\n",
      "CLEANED DATASET OVERVIEW\n",
      "============================================================\n",
      "Total tweets after cleaning: 329\n",
      "Removed: 79 tweets (19.4%)\n",
      "\n",
      "Dataset shape: (329, 14)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply full preprocessing pipeline\n",
    "df_clean = preprocessing.preprocess_pipeline(df, clean_text_content=False)\n",
    "\n",
    "# Display cleaned dataset info\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CLEANED DATASET OVERVIEW\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total tweets after cleaning: {len(df_clean)}\")\n",
    "print(f\"Removed: {len(df) - len(df_clean)} tweets ({((len(df) - len(df_clean))/len(df)*100):.1f}%)\")\n",
    "print(f\"\\nDataset shape: {df_clean.shape}\")\n",
    "print(f\"\\n{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned tweets:\n",
      "                                                                                                                                            Tweet Content  \\\n",
      "0                                                                                                    We tried to stop it from overthinking.\\n\\nWe failed.   \n",
      "5                                          I predicted this\\nThey need some one they can control without him battling and eye.\\nThey need a 'yes sir' man   \n",
      "10                            Well, hope this new coach meets the expectation of the board and fans. Big coaches tend to turn down the chelsea job offer.   \n",
      "11  Chelsea always sign coaches for 6 and half years before they'll fire them a year later after disastrous campaign. I pray he managed to see till De...   \n",
      "12                                                                                                I give him till the next international break, march max   \n",
      "14                                                                     What's the probability of him being sacked before june on a scale of 1-10? \\n@grok   \n",
      "16                                                                                                                        Win-win cooperation, surprises.   \n",
      "17                                                                                                    No way they gave this random dude 6 years contract    \n",
      "18                                                                                           6 years & half contract like they won’t sack him before 2027   \n",
      "19                                                                                 Six and a half years at Chelsea? bold. very bold. see you in november.   \n",
      "\n",
      "    engagement_score  tweet_length  word_count  \n",
      "0                 68            50           9  \n",
      "5                  1           108          21  \n",
      "10                 0           123          23  \n",
      "11                 3           195          35  \n",
      "12                 1            55          10  \n",
      "14                 3            81          15  \n",
      "16                 1            31           3  \n",
      "17                 1            51          10  \n",
      "18                 3            60          12  \n",
      "19                 3            70          14  \n",
      "\n",
      "\n",
      "Engagement Statistics:\n",
      "              Views         Likes     Retweets      Replies  engagement_score\n",
      "count  3.290000e+02    329.000000   329.000000   329.000000        329.000000\n",
      "mean   4.419459e+05    250.194529    32.975684    19.346505        374.185410\n",
      "std    6.539054e+06   3298.805537   518.883375   309.357706       5257.963755\n",
      "min    0.000000e+00      0.000000     0.000000     0.000000          0.000000\n",
      "25%    1.730000e+02      1.000000     0.000000     0.000000          2.000000\n",
      "50%    9.530000e+02      3.000000     0.000000     0.000000          5.000000\n",
      "75%    4.128000e+03     11.000000     0.000000     2.000000         21.000000\n",
      "max    1.179307e+08  59410.000000  9390.000000  5612.000000      95026.000000\n",
      "\n",
      "\n",
      "Text Statistics:\n",
      "       tweet_length  word_count  hashtag_count  mention_count\n",
      "count    329.000000  329.000000     329.000000     329.000000\n",
      "mean      70.896657   12.835866       0.006079       0.021277\n",
      "std       61.225206   10.320642       0.110264       0.144525\n",
      "min        3.000000    1.000000       0.000000       0.000000\n",
      "25%       33.000000    6.000000       0.000000       0.000000\n",
      "50%       51.000000   10.000000       0.000000       0.000000\n",
      "75%       84.000000   15.000000       0.000000       0.000000\n",
      "max      286.000000   53.000000       2.000000       1.000000\n"
     ]
    }
   ],
   "source": [
    "# Display sample cleaned tweets\n",
    "print(\"Sample cleaned tweets:\")\n",
    "print(df_clean[['Tweet Content', 'engagement_score', 'tweet_length', 'word_count']].head(10))\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\n\\nEngagement Statistics:\")\n",
    "print(df_clean[['Views', 'Likes', 'Retweets', 'Replies', 'engagement_score']].describe())\n",
    "\n",
    "print(f\"\\n\\nText Statistics:\")\n",
    "print(df_clean[['tweet_length', 'word_count', 'hashtag_count', 'mention_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check @grok Query Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of @grok queries (removed from dataset):\n",
      "1. @grok\n",
      " who is he?\n",
      "\n",
      "2. @grok\n",
      " make him bald and resemble Enzo Maresca.\n",
      "\n",
      "3. @grok\n",
      " remove Liam rosenior name and put Arnold Masoha\n",
      "\n",
      "4. @grok\n",
      " what is the likes count?\n",
      "\n",
      "5. @grok\n",
      " just lol  it will end tears\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample tweets that would have been filtered out\n",
    "print(\"Examples of @grok queries (removed from dataset):\")\n",
    "grok_queries = df[df['Tweet Content'].apply(preprocessing.is_grok_query)]['Tweet Content'].head(5)\n",
    "for i, tweet in enumerate(grok_queries, 1):\n",
    "    print(f\"{i}. {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Directories ensured:\n",
      "  - ./data/processed\n",
      "./outputs/figures\n",
      "./outputs/tables\n",
      "./outputs/models\n",
      "./outputs/metrics\n",
      "\n",
      "============================================================\n",
      "✓ Cleaned dataset saved to: /home/emmanuelabayor/projects/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior/data/processed/tweets_cleaned.csv\n",
      "  - Total tweets: 329\n",
      "  - Columns: ['Tweet Link', 'Author Handle', 'Tweet Content', 'Views', 'Likes', 'Retweets', 'Replies', 'Tweet Creation Date', 'Scraped Date', 'engagement_score', 'tweet_length', 'word_count', 'hashtag_count', 'mention_count']\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "utils.ensure_directories()\n",
    "\n",
    "# Save cleaned dataset\n",
    "output_path = utils.get_processed_data_path('tweets_cleaned.csv')\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"  - Total tweets: {len(df_clean)}\")\n",
    "print(f\"  - Columns: {list(df_clean.columns)}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: Apply Text Cleaning (for ML Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text cleaning...\n",
      "\n",
      "Sample tweets before and after cleaning:\n",
      "\n",
      "1. BEFORE: We tried to stop it from overthinking.\n",
      "\n",
      "We failed....\n",
      "   AFTER:  we tried to stop it from overthinking we failed...\n",
      "\n",
      "2. BEFORE: I predicted this\n",
      "They need some one they can control without him battling and eye.\n",
      "They need a 'yes ...\n",
      "   AFTER:  i predicted this they need some one they can control without him battling and eye they need a yes si...\n",
      "\n",
      "3. BEFORE: Well, hope this new coach meets the expectation of the board and fans. Big coaches tend to turn down...\n",
      "   AFTER:  well hope this new coach meets the expectation of the board and fans big coaches tend to turn down t...\n",
      "\n",
      "4. BEFORE: Chelsea always sign coaches for 6 and half years before they'll fire them a year later after disastr...\n",
      "   AFTER:  chelsea always sign coaches for 6 and half years before theyll fire them a year later after disastro...\n",
      "\n",
      "5. BEFORE: I give him till the next international break, march max...\n",
      "   AFTER:  i give him till the next international break march max...\n"
     ]
    }
   ],
   "source": [
    "# Apply text cleaning (remove URLs, mentions, special chars, etc.)\n",
    "print(\"Applying text cleaning...\")\n",
    "df_clean['Tweet Content Cleaned'] = df_clean['Tweet Content'].apply(preprocessing.clean_text)\n",
    "\n",
    "print(\"\\nSample tweets before and after cleaning:\")\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. BEFORE: {df_clean['Tweet Content'].iloc[i][:100]}...\")\n",
    "    print(f\"   AFTER:  {df_clean['Tweet Content Cleaned'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Dataset with Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ Dataset with cleaned text saved to: /home/emmanuelabayor/projects/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior/data/processed/tweets_cleaned_with_text.csv\n",
      "  - Total tweets: 329\n",
      "  - Columns: ['Tweet Link', 'Author Handle', 'Tweet Content', 'Views', 'Likes', 'Retweets', 'Replies', 'Tweet Creation Date', 'Scraped Date', 'engagement_score', 'tweet_length', 'word_count', 'hashtag_count', 'mention_count', 'Tweet Content Cleaned']\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save dataset with cleaned text\n",
    "output_path = utils.get_processed_data_path('tweets_cleaned_with_text.csv')\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Dataset with cleaned text saved to: {output_path}\")\n",
    "print(f\"  - Total tweets: {len(df_clean)}\")\n",
    "print(f\"  - Columns: {list(df_clean.columns)}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Data Preprocessing Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "- Original dataset: 621 tweets\n",
    "- Cleaned dataset: ~250-300 tweets (after filtering)\n",
    "- Removed: @grok queries, non-English, duplicates, empty tweets\n",
    "- Added: engagement features, text features\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "→ **`2_exploratory_analysis.ipynb`** - Perform EDA and visualizations\n",
    "\n",
    "**Saved Files:**\n",
    "- `data/processed/tweets_cleaned.csv`\n",
    "- `data/processed/tweets_cleaned_with_text.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
