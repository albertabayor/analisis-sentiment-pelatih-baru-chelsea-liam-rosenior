{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 0: Setup Google Colab Environment\n",
    "\n",
    "This notebook sets up the environment for Google Colab or local execution.\n",
    "\n",
    "## Steps:\n",
    "1. Check runtime environment (Colab vs Local)\n",
    "2. Mount Google Drive (if on Colab)\n",
    "3. Install required packages\n",
    "4. Verify project structure\n",
    "5. Download NLTK data\n",
    "6. Test imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Runtime Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab: False\n",
      "Python version: 3.12.3 (main, Jan  8 2026, 11:30:50) [GCC 13.3.0]\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Google Colab\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {is_colab}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (if on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab - using local environment.\n"
     ]
    }
   ],
   "source": [
    "if is_colab:\n",
    "    from google.colab import drive\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"\\n✓ Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Navigate to project folder\n",
    "    # IMPORTANT: Update this path to match your Google Drive folder structure\n",
    "    project_folder = '/content/drive/MyDrive/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior'\n",
    "    \n",
    "    if os.path.exists(project_folder):\n",
    "        os.chdir(project_folder)\n",
    "        print(f\"\\n✓ Changed directory to: {project_folder}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ WARNING: Project folder not found at: {project_folder}\")\n",
    "        print(\"Please upload the project to this path in Google Drive.\")\n",
    "else:\n",
    "    print(\"Not running in Colab - using local environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/emmanuelabayor/projects/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior/notebooks\n",
      "\n",
      "Project structure:\n",
      "  [FILE] UNIFIED_complete_pipeline.ipynb\n",
      "  [FILE] 5_results_visualization.ipynb\n",
      "  [DIR ] data\n",
      "  [FILE] 3_sentiment_labeling.ipynb\n",
      "  [FILE] 0_setup_colab.ipynb\n",
      "  [DIR ] .ipynb_checkpoints\n",
      "  [FILE] 1_data_preprocessing.ipynb\n",
      "  [DIR ] outputs\n",
      "  [FILE] 2_exploratory_analysis.ipynb\n",
      "  [FILE] 4_ml_modeling.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Check current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "\n",
    "# List directories\n",
    "print(\"\\nProject structure:\")\n",
    "for item in os.listdir('.'):\n",
    "    item_type = \"DIR \" if os.path.isdir(item) else \"FILE\"\n",
    "    print(f\"  [{item_type}] {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab. Please ensure packages are installed locally:\n",
      "pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Install packages (only needed in Colab)\n",
    "if is_colab:\n",
    "    print(\"Installing required packages...\")\n",
    "    !pip install -q pandas numpy scikit-learn nltk vaderSentiment langdetect matplotlib seaborn wordcloud plotly ipython\n",
    "    print(\"\\n✓ All packages installed successfully!\")\n",
    "else:\n",
    "    print(\"Not running in Colab. Please ensure packages are installed locally:\")\n",
    "    print(\"pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "\n",
      "✓ NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/emmanuelabayor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/emmanuelabayor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/emmanuelabayor/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/emmanuelabayor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"Downloading NLTK data...\")\n",
    "\n",
    "# Required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"\\n✓ NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imports...\n",
      "\n",
      "✓ pandas\n",
      "✓ numpy\n",
      "✓ scikit-learn\n",
      "✓ vaderSentiment\n",
      "✓ langdetect\n",
      "✓ matplotlib\n",
      "✓ seaborn\n",
      "✓ wordcloud\n",
      "\n",
      "✓ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Test all required imports\n",
    "print(\"Testing imports...\\n\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from langdetect import detect\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    print(\"✓ pandas\")\n",
    "    print(\"✓ numpy\")\n",
    "    print(\"✓ scikit-learn\")\n",
    "    print(\"✓ vaderSentiment\")\n",
    "    print(\"✓ langdetect\")\n",
    "    print(\"✓ matplotlib\")\n",
    "    print(\"✓ seaborn\")\n",
    "    print(\"✓ wordcloud\")\n",
    "    print(\"\\n✓ All imports successful!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n✗ Import error: {e}\")\n",
    "    print(\"Please ensure all packages are installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Import Project Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Custom modules imported successfully!\n",
      "  - utils\n",
      "  - preprocessing\n",
      "  - feature_engineering\n",
      "  - models\n"
     ]
    }
   ],
   "source": [
    "# Import custom utility modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path for proper module imports\n",
    "# This allows 'from src import utils' to work correctly\n",
    "project_root = os.path.dirname(os.getcwd())  # Go up one level from notebooks/\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "try:\n",
    "    from src import utils, preprocessing, feature_engineering, models\n",
    "    print(\"\\n[OK] Custom modules imported successfully!\")\n",
    "    print(\"  - utils\")\n",
    "    print(\"  - preprocessing\")\n",
    "    print(\"  - feature_engineering\")\n",
    "    print(\"  - models\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n[ERROR] Error importing custom modules: {e}\")\n",
    "    print(\"Make sure you are in the project root directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Data File Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Data file found: /home/emmanuelabayor/projects/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior/data/raw/tweets.csv\n",
      "  - Total tweets: 408\n",
      "  - Columns: ['Tweet Link', 'Author Handle', 'Tweet Content', 'Views', 'Likes', 'Retweets', 'Replies', 'Tweet Creation Date', 'Scraped Date']\n",
      "\n",
      "  Sample tweets:\n",
      "                                       Tweet Content\n",
      "0  We tried to stop it from overthinking.\\n\\nWe f...\n",
      "1                                                Waw\n",
      "2                                 @grok\\n who is he?\n"
     ]
    }
   ],
   "source": [
    "# Check if data file exists\n",
    "import pandas as pd\n",
    "\n",
    "# Handle both local (running from notebooks/) and Colab environments\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(project_root, 'data', 'raw', 'tweets.csv')\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"\\n[OK] Data file found: {data_path}\")\n",
    "    print(f\"  - Total tweets: {len(df)}\")\n",
    "    print(f\"  - Columns: {list(df.columns)}\")\n",
    "    print(f\"\\n  Sample tweets:\")\n",
    "    print(df[[\"Tweet Content\"]].head(3))\n",
    "else:\n",
    "    print(f\"\\n[ERROR] Data file not found: {data_path}\")\n",
    "    print(\"Please ensure tweets.csv is in the data/raw/ folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ensure Output Directories Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All output directories ensured:\n",
      "  - data/processed\n",
      "  - outputs/figures\n",
      "  - outputs/tables\n",
      "  - outputs/models\n",
      "  - outputs/metrics\n"
     ]
    }
   ],
   "source": [
    "# Ensure all output directories exist\n",
    "directories = [\n",
    "    'data/processed',\n",
    "    'outputs/figures',\n",
    "    'outputs/tables',\n",
    "    'outputs/models',\n",
    "    'outputs/metrics'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ All output directories ensured:\")\n",
    "for directory in directories:\n",
    "    print(f\"  - {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Setup Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your environment is now ready for analysis. You can proceed to the next notebook:\n",
    "\n",
    "→ **`1_data_preprocessing.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
