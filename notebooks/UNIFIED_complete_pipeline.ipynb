{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Sentiment Analysis - Liam Rosenior Chelsea Appointment\n",
    "\n",
    "**Complete Pipeline: Data Preprocessing â†’ EDA â†’ Sentiment Labeling â†’ ML Modeling**\n",
    "\n",
    "---\n",
    "\n",
    "## How to Run on Google Colab\n",
    "\n",
    "1. **Open Google Colab**: https://colab.research.google.com\n",
    "\n",
    "2. **Clone the repository**:\n",
    "```python\n",
    "!git clone https://github.com/albertabayor/analisis-sentiment-pelatih-baru-chelsea-liam-rosenior.git\n",
    "%cd analisis-sentiment-pelatih-baru-chelsea-liam-rosenior\n",
    "```\n",
    "\n",
    "3. **Mount Google Drive** (optional - for saving outputs):\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "4. **Install dependencies**:\n",
    "```python\n",
    "!pip install -q pandas numpy scikit-learn nltk vaderSentiment langdetect matplotlib seaborn wordcloud plotly\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "```\n",
    "\n",
    "5. **Run this notebook** by pressing `Shift+Enter` on each cell\n",
    "\n",
    "---\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "- **Dataset**: 621 tweets about Liam Rosenior's appointment as Chelsea FC head coach\n",
    "- **After Cleaning**: ~329 tweets (filtered @grok queries, non-English)\n",
    "- **Sentiment Analysis**: VADER\n",
    "- **ML Models**: Logistic Regression, Multinomial Naive Bayes\n",
    "- **Best Model**: Logistic Regression (~57.58% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy scikit-learn nltk vaderSentiment langdetect matplotlib seaborn wordcloud plotly joblib\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"[OK] Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from langdetect import detect, LangDetectException\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import joblib\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('outputs/figures', exist_ok=True)\n",
    "os.makedirs('outputs/tables', exist_ok=True)\n",
    "os.makedirs('outputs/models', exist_ok=True)\n",
    "os.makedirs('outputs/metrics', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "print(\"[OK] Libraries imported and directories created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PREPROCESSING FUNCTIONS ====================\n",
    "\n",
    "def is_grok_query(tweet_content):\n",
    "    \"\"\"Check if a tweet is a question TO @grok\"\"\"\n",
    "    if not isinstance(tweet_content, str):\n",
    "        return False\n",
    "    tweet_lower = tweet_content.lower().strip()\n",
    "    if tweet_lower.startswith('@grok'):\n",
    "        return True\n",
    "    if '@grok' in tweet_lower:\n",
    "        question_words = ['who is', 'what is', 'where', 'when', 'why', 'how', 'can you', 'tell me', 'explain']\n",
    "        for qw in question_words:\n",
    "            if qw in tweet_lower:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Detect if text is in English\"\"\"\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean tweet text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    print(f\"Original tweets: {len(df)}\")\n",
    "    \n",
    "    # Remove empty tweets\n",
    "    df = df.dropna(subset=['Tweet Content'])\n",
    "    df = df[df['Tweet Content'].str.strip() != '']\n",
    "    print(f\"After removing empty: {len(df)}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['Tweet Content'], keep='first')\n",
    "    print(f\"After removing duplicates: {len(df)}\")\n",
    "    \n",
    "    # Filter @grok queries\n",
    "    is_grok_mask = df['Tweet Content'].apply(is_grok_query)\n",
    "    df = df[~is_grok_mask]\n",
    "    print(f\"After removing @grok queries: {len(df)}\")\n",
    "    \n",
    "    # Filter English only\n",
    "    is_english_mask = df['Tweet Content'].apply(is_english)\n",
    "    df = df[is_english_mask]\n",
    "    print(f\"After filtering English: {len(df)}\")\n",
    "    \n",
    "    # Add engagement features\n",
    "    numeric_cols = ['Views', 'Likes', 'Retweets', 'Replies']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    df['engagement_score'] = df['Likes'] + (df['Retweets'] * 2) + (df['Replies'] * 3)\n",
    "    df['tweet_length'] = df['Tweet Content'].apply(lambda x: len(str(x)))\n",
    "    df['word_count'] = df['Tweet Content'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Clean text\n",
    "    df['Tweet Content Cleaned'] = df['Tweet Content'].apply(clean_text)\n",
    "    \n",
    "    print(f\"\\n[OK] Preprocessing complete! Final: {len(df)} tweets\")\n",
    "    return df\n",
    "\n",
    "print(\"[OK] Preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "if os.path.exists('data/raw/tweets.csv'):\n",
    "    df = pd.read_csv('data/raw/tweets.csv')\n",
    "    print(f\"Loaded {len(df)} tweets from data/raw/tweets.csv\")\n",
    "else:\n",
    "    # Create sample data if file doesn't exist\n",
    "    print(\"Creating sample dataset...\")\n",
    "    sample_data = {\n",
    "        'Tweet Content': [\n",
    "            \"Great appointment for Chelsea! Liam Rosenior is the right choice!\",\n",
    "            \"I'm skeptical about this appointment. Has he proven himself?\",\n",
    "            \"Neutral opinion. Let's wait and see how it goes.\",\n",
    "            \"@grok who is Liam Rosenior?\",\n",
    "            \"Chelsea making another questionable decision.\"\n",
    "        ] * 100  # Multiply for demo\n",
    "    }\n",
    "    df = pd.DataFrame(sample_data)\n",
    "    # Add random engagement metrics\n",
    "    np.random.seed(42)\n",
    "    df['Views'] = np.random.randint(100, 10000, len(df))\n",
    "    df['Likes'] = np.random.randint(0, 500, len(df))\n",
    "    df['Retweets'] = np.random.randint(0, 100, len(df))\n",
    "    df['Replies'] = np.random.randint(0, 50, len(df))\n",
    "    print(f\"Created sample dataset: {len(df)} tweets\")\n",
    "\n",
    "# Preprocess data\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('data/processed/tweets_cleaned.csv', index=False)\n",
    "print(\"Saved to data/processed/tweets_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VADER sentiment analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "print(\"Applying VADER sentiment analysis...\")\n",
    "vader_scores = df['Tweet Content'].apply(get_vader_sentiment)\n",
    "\n",
    "df['vader_neg'] = vader_scores.apply(lambda x: x['neg'])\n",
    "df['vader_neu'] = vader_scores.apply(lambda x: x['neu'])\n",
    "df['vader_pos'] = vader_scores.apply(lambda x: x['pos'])\n",
    "df['vader_compound'] = vader_scores.apply(lambda x: x['compound'])\n",
    "\n",
    "# Categorize sentiment\n",
    "def categorize_sentiment(compound):\n",
    "    if compound >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['sentiment'] = df['vader_compound'].apply(categorize_sentiment)\n",
    "\n",
    "# Save labeled data\n",
    "df.to_csv('data/processed/tweets_labeled.csv', index=False)\n",
    "\n",
    "# Print distribution\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SENTIMENT DISTRIBUTION\")\n",
    "print(\"=\"*50)\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "for sent, count in sentiment_counts.items():\n",
    "    pct = count/len(df)*100\n",
    "    print(f\"{sent}: {count} ({pct:.1f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plot distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "colors = ['#2ecc71', '#95a5a6', '#ff6b6b']\n",
    "sentiment_counts.plot(kind='pie', autopct='%1.1f%%', colors=colors, ax=ax, startangle=90)\n",
    "ax.set_title('Sentiment Distribution - Liam Rosenior Appointment', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Sentiment analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "print(\"Generating WordCloud...\")\n",
    "all_text = ' '.join(df['Tweet Content'].astype(str).tolist())\n",
    "wordcloud = WordCloud(width=1200, height=600, background_color='white',\n",
    "                      stopwords=STOPWORDS, max_words=100, colormap='viridis').generate(all_text)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis('off')\n",
    "ax.set_title('WordCloud - Most Frequent Words', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Engagement by Sentiment\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.boxplot(x='sentiment', y='engagement_score', data=df,\n",
    "            order=['Negative', 'Neutral', 'Positive'], palette='Set2', ax=ax)\n",
    "ax.set_title('Engagement Score by Sentiment', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/engagement_by_sentiment.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] EDA complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df['Tweet Content Cleaned']\n",
    "y = df['sentiment']\n",
    "label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y_encoded = y.map(label_map)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} tweets\")\n",
    "print(f\"Test set: {len(X_test)} tweets\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\nCreating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2), stop_words='english', min_df=2, max_df=0.95)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "\n",
    "# Count Vectorization\n",
    "print(\"\\nCreating Count features...\")\n",
    "count_vec = CountVectorizer(max_features=500, stop_words='english', min_df=2, max_df=0.95)\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)\n",
    "print(f\"Count features: {X_train_count.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING LOGISTIC REGRESSION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=200, C=1.0, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "lr_recall = recall_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\")\n",
    "print(f\"F1-Score: {lr_f1:.4f}\")\n",
    "\n",
    "# Train Naive Bayes\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING MULTINOMIAL NAIVE BAYES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred_nb = nb.predict(X_test_count)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "nb_precision = precision_score(y_test, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_recall = recall_score(y_test, y_pred_nb, average='weighted', zero_division=0)\n",
    "nb_f1 = f1_score(y_test, y_pred_nb, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"F1-Score: {nb_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Multinomial Naive Bayes'],\n",
    "    'Accuracy': [lr_accuracy, nb_accuracy],\n",
    "    'Precision': [lr_precision, nb_precision],\n",
    "    'Recall': [lr_recall, nb_recall],\n",
    "    'F1-Score': [lr_f1, nb_f1]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_idx = comparison['Accuracy'].idxmax()\n",
    "best_model = comparison.loc[best_idx, 'Model']\n",
    "print(f\"\\n[OK] Best Model: {best_model} (Accuracy: {comparison.loc[best_idx, 'Accuracy']:.4f})\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(2)\n",
    "width = 0.2\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, comparison[metric], width, label=metric)\n",
    "ax.set_xticks(x + width*1.5)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=15)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save comparison\n",
    "comparison.to_csv('outputs/tables/model_comparison.csv', index=False)\n",
    "print(\"\\n[OK] Model comparison saved to outputs/tables/model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# LR Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Logistic Regression Confusion Matrix')\n",
    "\n",
    "# NB Confusion Matrix\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
    "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Naive Bayes Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "joblib.dump(lr, 'outputs/models/logistic_regression.pkl')\n",
    "joblib.dump(nb, 'outputs/models/naive_bayes.pkl')\n",
    "joblib.dump(tfidf, 'outputs/models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(count_vec, 'outputs/models/count_vectorizer.pkl')\n",
    "\n",
    "print(\"[OK] Models saved to outputs/models/\")\n",
    "\n",
    "# Save final summary\n",
    "summary = {\n",
    "    'total_tweets': len(df),\n",
    "    'positive_tweets': (df['sentiment']=='Positive').sum(),\n",
    "    'neutral_tweets': (df['sentiment']=='Neutral').sum(),\n",
    "    'negative_tweets': (df['sentiment']=='Negative').sum(),\n",
    "    'lr_accuracy': lr_accuracy,\n",
    "    'nb_accuracy': nb_accuracy,\n",
    "    'best_model': best_model\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('outputs/tables/final_summary.csv', index=False)\n",
    "\n",
    "print(\"[OK] Final summary saved to outputs/tables/final_summary.csv\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Tweets Analyzed: {len(df)}\")\n",
    "print(f\"  - Positive: {summary['positive_tweets']} ({summary['positive_tweets']/len(df)*100:.1f}%)\")\n",
    "print(f\"  - Neutral: {summary['neutral_tweets']} ({summary['neutral_tweets']/len(df)*100:.1f}%)\")\n",
    "print(f\"  - Negative: {summary['negative_tweets']} ({summary['negative_tweets']/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸŽ‰ ANALYSIS COMPLETE!\")\n",
    "print(\"\\nOutput files saved to:\")\n",
    "- outputs/figures/ (visualizations)\n",
    "- outputs/tables/ (CSV tables)\n",
    "- outputs/models/ (trained models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Output Files\n",
    "\n",
    "All outputs are saved in the `outputs/` folder:\n",
    "\n",
    "**Figures:**\n",
    "- `sentiment_distribution.png` - Pie chart of sentiment\n",
    "- `wordcloud.png` - Word frequency visualization\n",
    "- `engagement_by_sentiment.png` - Engagement analysis\n",
    "- `model_comparison.png` - Model performance comparison\n",
    "- `confusion_matrices.png` - Confusion matrices for both models\n",
    "\n",
    "**Tables:**\n",
    "- `model_comparison.csv` - Model performance metrics\n",
    "- `final_summary.csv` - Overall analysis summary\n",
    "\n",
    "**Models:**\n",
    "- `logistic_regression.pkl` - Trained LR model\n",
    "- `naive_bayes.pkl` - Trained NB model\n",
    "- `tfidf_vectorizer.pkl` - TF-IDF vectorizer\n",
    "- `count_vectorizer.pkl` - Count vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
